{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T04:09:35.877585Z",
     "start_time": "2018-04-13T04:09:34.156624Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T04:09:36.521406Z",
     "start_time": "2018-04-13T04:09:36.514334Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def augmentation(x, max_offset=2):\n",
    "    bz, h, w, c = x.shape\n",
    "    bg = np.zeros([bz, w + 2 * max_offset, h + 2 * max_offset, c])\n",
    "    offsets = np.random.randint(0, 2 * max_offset + 1, 2)\n",
    "    #shift\n",
    "    bg[:, offsets[0]:offsets[0] + h, offsets[1]:offsets[1] + w, :] = x\n",
    "    return bg[:, max_offset:max_offset + h, max_offset:max_offset + w, :]\n",
    "\n",
    "def multimnist_test_iter(iters=1000, batch_size=32, is_shift_ag=True):\n",
    "    mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "    max_offset = int(is_shift_ag) * 2\n",
    "    for i in range(iters):\n",
    "        #two batch at one iter\n",
    "        batch1 = mnist.test.next_batch(batch_size)\n",
    "        batch2 = mnist.test.next_batch(batch_size)\n",
    "        images1 = augmentation(batch1[0].reshape([batch_size, 28, 28, 1]),\n",
    "                               max_offset)\n",
    "        images2 = augmentation(batch2[0].reshape([batch_size, 28, 28, 1]),\n",
    "                               max_offset)\n",
    "        #overlapping\n",
    "        images = np.logical_or(images1, images2).astype(np.float32)\n",
    "        images = np.concatenate([images, images1, images2], axis=-1)\n",
    "        y1, y2 = batch1[1], batch2[1]\n",
    "        y0 = np.logical_or(y1, y2).astype(np.float32)\n",
    "        yield images, np.stack([y0, y1, y2], axis=-1)\n",
    "        #size of result is 32*28*28*3, here data in same pos of pics with same index, will be put into one inner array\n",
    "        #sequence of shift and concatenate is reversible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T04:09:37.085898Z",
     "start_time": "2018-04-13T04:09:37.051859Z"
    },
    "code_folding": [
     1,
     24,
     26,
     29,
     41,
     94,
     250
    ]
   },
   "outputs": [],
   "source": [
    "class CapsNet(object):\n",
    "    def __init__(self,\n",
    "                 routing_iterations=3,\n",
    "                 batch_size=128,\n",
    "                 is_multi_mnist=True,\n",
    "                 steps=5000,\n",
    "                 norm=True,\n",
    "                 lr_find=False):\n",
    "        \"\"\"\n",
    "        routing_iterations: iterations for routing in CapsNet\n",
    "        batch_size: data size for every batch\n",
    "        is_multi_mnist: if it's for single or overlapping image\n",
    "        steps: epochs \n",
    "        norm: if batch_normalization or not\n",
    "        \"\"\"\n",
    "        self.iterations = routing_iterations\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        #Single or overlapping image\n",
    "        self.is_multi_mnist = float(is_multi_mnist)\n",
    "\n",
    "        self.x = tf.placeholder(tf.float32, [None, 28, 28, 3])\n",
    "        self.y = tf.placeholder(tf.float32, [None, 10, 3])\n",
    "        \n",
    "        if lr_find:\n",
    "            self.lr = tf.placeholder(tf.float32)\n",
    "        else:\n",
    "            #use a exponentially decayed learninig rate\n",
    "            global_step = tf.Variable(0)\n",
    "            lr = tf.train.exponential_decay(\n",
    "                0.00075, global_step, steps / 10, 0.96, staircase=False)\n",
    "        self.norm = norm\n",
    "        self.on_train = tf.placeholder(tf.bool)\n",
    "\n",
    "        x_composed, x_a, x_b = tf.split(self.x, num_or_size_splits=3, axis=3)\n",
    "        y_composed, y_a, y_b = tf.split(self.y, num_or_size_splits=3, axis=2)\n",
    "    \n",
    "        #if not overlapping, mask = tf.ones_like(y_composed[:,0,0])\n",
    "        #if overlapping: mask = tf.reduce_sum(y_composed, axis=[1,2]) - 1.0\n",
    "        #for not overlapping: ones_like will generate the array of 1 the same size as input array. Here y_composed[:,0,0], each pic choose only 1 pos, therefore, it is a 1 dimension vector, the size equals batch_size.\n",
    "        #for overlapping: reduce_sum will sum all num in a range. Here, we choose [1, 2], which is all label for a particular pic. Therefore, reduce_sum here will sum label for each pic in the batch, will represent the num of digits in each pic.\n",
    "        valid_mask = self.is_multi_mnist * (tf.reduce_sum(y_composed, axis=[1,2]) - 1.0) \\\n",
    "                      + (1.0 - self.is_multi_mnist) * tf.ones_like(y_composed[:,0,0])\n",
    "        \n",
    "        #reg_term is the penalty term\n",
    "        v_digit,reg_term = self.get_CapsNet(x_composed, self.norm, self.on_train)\n",
    "\n",
    "        length_v = tf.reduce_sum(v_digit**2.0, axis=-1)**0.5\n",
    "\n",
    "        x_rec_a = self.get_mlp_decoder(v_digit * y_a)\n",
    "        x_rec_b = self.get_mlp_decoder(v_digit * y_b, reuse=True)\n",
    "        loss_rec_a = tf.reduce_sum((x_rec_a - x_a)**2.0, axis=[1, 2, 3])\n",
    "        loss_rec_b = tf.reduce_sum((x_rec_b - x_b)**2.0, axis=[1, 2, 3])\n",
    "        self.loss_rec = (loss_rec_a + loss_rec_b) / 2.0\n",
    "        \n",
    "        self.loss_cls = tf.reduce_sum(\n",
    "            y_composed[:, :, 0] * tf.maximum(0.0, 0.9 - length_v)**2.0 + 0.5 *\n",
    "            (1.0 - y_composed[:, :, 0]) * tf.maximum(0.0, length_v - 0.1)**2.0,\n",
    "            axis=-1)\n",
    "        self.loss_cls = tf.reduce_sum(\n",
    "            self.loss_cls * valid_mask) / tf.reduce_sum(valid_mask)\n",
    "        #loss_cls: batch_size, valid_mask: _batch_size\n",
    "        #sum1: loss for all pics in the batch\n",
    "        #sum2: num of digits for all pics in the batch\n",
    "        #cal the loss percentage considering y_composed (real label)\n",
    "        \n",
    "        self.loss_rec = tf.reduce_sum(\n",
    "            self.loss_rec * valid_mask) / tf.reduce_sum(valid_mask)\n",
    "        #loss_rec: batch_size, valid_mask: batch_size\n",
    "        #sum1: loss for all pics in the batch\n",
    "        #cal the loss percentage considering prediction from x_composed, x_a, x_b\n",
    "        \n",
    "        #loss with the penalty term\n",
    "        self.loss = self.loss_cls + 0.0005 * self.loss_rec + reg_term\n",
    "        \n",
    "        #adam optimizer\n",
    "        if lr_find:\n",
    "            self.train = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
    "        else:\n",
    "            self.train = tf.train.AdamOptimizer(\n",
    "                learning_rate=lr).minimize(\n",
    "                    self.loss, global_step=global_step)\n",
    "\n",
    "        #accuracy for single and overlapping training data\n",
    "        if is_multi_mnist:\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(tf.nn.in_top_k(length_v,tf.argmax(tf.squeeze(y_a), 1),k=2),tf.float32))+\\\n",
    "                            tf.reduce_mean(tf.cast(tf.nn.in_top_k(length_v,tf.argmax(tf.squeeze(y_b), 1),k=2),tf.float32))\n",
    "            self.accuracy /= 2.0\n",
    "        else:\n",
    "            correct_prediction = tf.equal(\n",
    "                tf.argmax(y_composed[:, :, 0], 1), tf.argmax(length_v, 1))\n",
    "            self.accuracy = tf.reduce_mean(\n",
    "                tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def get_CapsNet(self, x, norm, on_train, reuse=False):\n",
    "        \"\"\"\n",
    "        norm: to use batch-normalization or not\n",
    "        on_train: if it's for train or test\n",
    "        \"\"\"\n",
    "        #Two conv layers and a capsule layer\n",
    "        with tf.variable_scope('CapsNet', reuse=reuse):\n",
    "            wconv1 = tf.get_variable(\n",
    "                'wconv1', [9, 9, 1, 256],\n",
    "                initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "            bconv1 = tf.get_variable(\n",
    "                'bconv1', [256],\n",
    "                initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "            wconv2 = tf.get_variable(\n",
    "                'wconv2', [9, 9, 256, 8 * 32],\n",
    "                initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "            bconv2 = tf.get_variable(\n",
    "                'bconv2', [8 * 32],\n",
    "                initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "            wcap = tf.get_variable(\n",
    "                'wcap', [1, 6, 6, 32, 8, 10, 16],\n",
    "                initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "            b = tf.get_variable(\n",
    "                'coupling_coefficient_logits', [1, 6, 6, 32, 1, 10, 1],\n",
    "                initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "        #use stop_gradient for debug\n",
    "        c = tf.stop_gradient(tf.nn.softmax(b, axis=5))\n",
    "        \n",
    "        #L2-regularization\n",
    "        tf.add_to_collection(tf.GraphKeys.WEIGHTS, wconv1)\n",
    "        tf.add_to_collection(tf.GraphKeys.WEIGHTS, wconv2)\n",
    "        tf.add_to_collection(tf.GraphKeys.WEIGHTS, wcap)\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=5.0/50000)\n",
    "        reg_term = tf.contrib.layers.apply_regularization(regularizer)\n",
    "\n",
    "        \n",
    "        if norm:\n",
    "            # BN for the first input\n",
    "            fc_mean, fc_var = tf.nn.moments(\n",
    "                x,\n",
    "                axes=[0, 1, 2],\n",
    "            )\n",
    "            scale = tf.Variable(tf.ones([1]))\n",
    "            shift = tf.Variable(tf.zeros([1]))\n",
    "            epsilon = 0.001\n",
    "            ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "            def mean_var_with_update():\n",
    "                \"\"\"\n",
    "                update mean and variance using moving average\n",
    "                \"\"\"\n",
    "                ema_apply_op = ema.apply([fc_mean, fc_var])\n",
    "                with tf.control_dependencies([ema_apply_op]):\n",
    "                    return tf.identity(fc_mean), tf.identity(fc_var)\n",
    "            \n",
    "            # if it's for test, then don't update mean and var, instead use the fc_mean and fc_var that already calculated\n",
    "            mean, var = tf.cond(\n",
    "                on_train, mean_var_with_update,\n",
    "                lambda: (ema.average(fc_mean), ema.average(fc_var)))\n",
    "            x = tf.nn.batch_normalization(x, mean, var, shift, scale, epsilon)\n",
    "\n",
    "        conv1 = tf.nn.conv2d(x, wconv1, [1, 1, 1, 1], padding='VALID') + bconv1\n",
    "\n",
    "        if norm:\n",
    "            # BN for the first conv layer\n",
    "            fc_mean, fc_var = tf.nn.moments(\n",
    "                conv1,\n",
    "                axes=[0, 1, 2],\n",
    "            )\n",
    "            scale = tf.Variable(tf.ones([1]))\n",
    "            shift = tf.Variable(tf.zeros([1]))\n",
    "            epsilon = 0.001\n",
    "            ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "            def mean_var_with_update():\n",
    "                ema_apply_op = ema.apply([fc_mean, fc_var])\n",
    "                with tf.control_dependencies([ema_apply_op]):\n",
    "                    return tf.identity(fc_mean), tf.identity(fc_var)\n",
    "\n",
    "            mean, var = tf.cond(\n",
    "                on_train, mean_var_with_update,\n",
    "                lambda: (ema.average(fc_mean), ema.average(fc_var)))\n",
    "            conv1 = tf.nn.batch_normalization(conv1, mean, var, shift, scale,\n",
    "                                              epsilon)\n",
    "\n",
    "        conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "        s_primary = tf.nn.conv2d(\n",
    "            conv1, wconv2, [1, 2, 2, 1], padding='VALID') + bconv2\n",
    "\n",
    "        if norm:\n",
    "            # BN for the second conv layer\n",
    "            fc_mean, fc_var = tf.nn.moments(\n",
    "                s_primary,\n",
    "                axes=[0, 1, 2],\n",
    "            )\n",
    "            scale = tf.Variable(tf.ones([1]))\n",
    "            shift = tf.Variable(tf.zeros([1]))\n",
    "            epsilon = 0.001\n",
    "            ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "            def mean_var_with_update():\n",
    "                ema_apply_op = ema.apply([fc_mean, fc_var])\n",
    "                with tf.control_dependencies([ema_apply_op]):\n",
    "                    return tf.identity(fc_mean), tf.identity(fc_var)\n",
    "\n",
    "            mean, var = tf.cond(\n",
    "                on_train, mean_var_with_update,\n",
    "                lambda: (ema.average(fc_mean), ema.average(fc_var)))\n",
    "            s_primary = tf.nn.batch_normalization(s_primary, mean, var, shift,\n",
    "                                                  scale, epsilon)\n",
    "\n",
    "        s_primary = tf.reshape(s_primary, [-1, 6, 6, 32, 8, 1, 1])\n",
    "        #change form from batch_size*6*6*256 into -1*6*6*32*8*1*1. For input of capsule, it contains 6*6*32 vectors, each vector has 8 elements. We could consider it as reshape the 256 output layer into 8 groups, each group contains 32 layer. Put 8 groups parallel and each vector is the pix in same positions in 8 groups. \n",
    "        #the use of 1, 1 will make it easy to cal when we do u*wcap in the following step. (* is the multiply by elements in corresponding pos) (expand automatically, but we need to keep same dimensions first)\n",
    "\n",
    "        v_primary = self.squash(s_primary, axis=4)\n",
    "\n",
    "        #CAPSNET\n",
    "        u = v_primary #input of capsule\n",
    "        u_ = tf.reduce_sum(u * wcap, axis=[4], keepdims=True)\n",
    "        s = tf.reduce_sum(u_ * c, axis=[1, 2, 3], keepdims=True)\n",
    "        v = self.squash(s, axis=-1)\n",
    "\n",
    "        if norm:\n",
    "            # BN for the capsule layer\n",
    "            fc_mean, fc_var = tf.nn.moments(\n",
    "                v,\n",
    "                axes=[0, 1, 2],\n",
    "            )\n",
    "            scale = tf.Variable(tf.ones([1]))\n",
    "            shift = tf.Variable(tf.zeros([1]))\n",
    "            epsilon = 0.001\n",
    "            ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "            def mean_var_with_update():\n",
    "                ema_apply_op = ema.apply([fc_mean, fc_var])\n",
    "                with tf.control_dependencies([ema_apply_op]):\n",
    "                    return tf.identity(fc_mean), tf.identity(fc_var)\n",
    "\n",
    "            mean, var = tf.cond(\n",
    "                on_train, mean_var_with_update,\n",
    "                lambda: (ema.average(fc_mean), ema.average(fc_var)))\n",
    "            v = tf.nn.batch_normalization(v, mean, var, shift, scale, epsilon)\n",
    "\n",
    "        for i in range(self.iterations - 1):\n",
    "            b += tf.reduce_sum(u_ * v, axis=-1, keepdims=True)\n",
    "            c = tf.nn.softmax(b, axis=5)\n",
    "            s = tf.reduce_sum(u_ * c, axis=[1, 2, 3], keepdims=True)\n",
    "            v = self.squash(s, axis=-1) #get the correct v\n",
    "\n",
    "        v_digit = tf.squeeze(v)\n",
    "\n",
    "        return v_digit,reg_term\n",
    "\n",
    "    def get_mlp_decoder(self, h, num_h=[10 * 16, 512, 1024, 784], reuse=False):\n",
    "        \"\"\"\n",
    "        restruction the input images: 160 -> 512 -> 1024 -> 784\n",
    "        \"\"\"\n",
    "        h = tf.reshape(h, [-1, 10 * 16])\n",
    "        with tf.variable_scope('decoder', reuse=reuse):\n",
    "            weights = []\n",
    "            for i in range(len(num_h) - 1):\n",
    "                w = tf.get_variable(\n",
    "                    'wfc%d' % i, [num_h[i], num_h[i + 1]],\n",
    "                    initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "                b = tf.get_variable(\n",
    "                    'bfc%d' % i, [num_h[i + 1]],\n",
    "                    initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "                weights.append((w, b))\n",
    "                if i < len(num_h) - 2:\n",
    "                    h = tf.nn.relu(tf.add(tf.matmul(h, w), b))\n",
    "                else:\n",
    "                    h = tf.nn.sigmoid(tf.add(tf.matmul(h, w), b))\n",
    "        x_rec = tf.reshape(h, [-1, 28, 28, 1])\n",
    "        return x_rec\n",
    "\n",
    "    def squash(self, s, axis=-1):\n",
    "        length_s = tf.reduce_sum(s**2.0, axis=axis, keepdims=True)**0.5\n",
    "        v = s * length_s / (1.0 + length_s**2.0) #shorten the length of each vector, make the length a little shorter than 1. Non-linear. To avoid the existence of 0 vector.\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T04:09:37.406693Z",
     "start_time": "2018-04-13T04:09:37.403597Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "is_multi_mnist = True\n",
    "is_shift_ag = True\n",
    "irun = 0\n",
    "steps = 1\n",
    "lr_find = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T04:09:38.129795Z",
     "start_time": "2018-04-13T04:09:38.126426Z"
    }
   },
   "outputs": [],
   "source": [
    "#test_iter = multimnist_test_iter(iters=steps, batch_size=batch_size, is_shift_ag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T04:09:40.200141Z",
     "start_time": "2018-04-13T04:09:38.574330Z"
    }
   },
   "outputs": [],
   "source": [
    "net = CapsNet(is_multi_mnist=is_multi_mnist, steps=steps, lr_find=lr_find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess,tf.train.latest_checkpoint('./checkpoint_dir'))\n",
    "ACC_TEST = sess.run(\n",
    "    net.accuracy,\n",
    "    feed_dict={\n",
    "        net.x: X_TEST,\n",
    "        net.y: Y_TEST,\n",
    "        net.on_train: False\n",
    "    })\n",
    "print(irun, ACC_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T04:09:43.334856Z",
     "start_time": "2018-04-13T04:09:40.656609Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoint_dir/MyModel\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "0 0.8046875\n"
     ]
    }
   ],
   "source": [
    "#sess = tf.Session()\n",
    "#saver = tf.train.Saver()\n",
    "#saver.restore(sess,tf.train.latest_checkpoint('./checkpoint_dir'))\n",
    "#for X_TEST,Y_TEST in test_iter:\n",
    "#    ACC_TEST = sess.run(\n",
    "#        net.accuracy,\n",
    "#        feed_dict={\n",
    "#            net.x: X_TEST,\n",
    "#            net.y: Y_TEST,\n",
    "#            net.on_train: False\n",
    "#        })\n",
    "#    print(irun, ACC_TEST)\n",
    "#    irun += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
